/**
 * Voice Pipeline
 * Main orchestrator: STT → LLM → TTS
 *
 * STT and TTS are optional - omit them if the client handles them locally.
 * Supports tool registration for function calling with any LLM backend.
 *
 * The pipeline is stateless - callers manage conversation history via ConversationContext.
 */

import type {
  STTPipeline,
  LLMPipeline,
  TTSPipeline,
  Message,
  ProgressCallback,
  AudioPlayable,
  Tool,
  ToolDefinition,
  ToolCall,
  AssistantMessage,
  ToolMessage,
} from './types';
import { TextNormalizer } from './services/text-normalizer';

/** Maximum number of tool call iterations to prevent infinite loops */
const MAX_TOOL_ITERATIONS = 10;

/** Default filler phrases while executing tools */
const DEFAULT_TOOL_FILLER_PHRASES = [
  'Let me check that for you.',
  'One moment please.',
  'Let me look that up.',
];

export interface VoicePipelineConfig {
  /** STT backend (optional if client does local STT) */
  stt?: STTPipeline | null;
  /** LLM backend (required) */
  llm: LLMPipeline;
  /** TTS backend (optional if client does local TTS) */
  tts?: TTSPipeline | null;
  /** System prompt for the LLM */
  systemPrompt: string;
  /** Registered tools for function calling */
  tools?: Tool[];
  /**
   * Filler phrases to say while executing tools.
   * Set to empty array to disable filler phrases.
   * @default ["Let me check that for you.", "One moment please.", "Let me look that up."]
   */
  toolFillerPhrases?: string[];
}

export interface VoicePipelineCallbacks {
  onTranscript: (text: string) => void;
  onResponseChunk: (text: string) => void;
  onAudio: (playable: AudioPlayable) => void;
  onComplete: () => void;
  onError: (error: Error) => void;
  /** Called when a tool is being executed */
  onToolCall?: (toolCall: ToolCall) => void;
  /** Called when a tool execution completes */
  onToolResult?: (toolCallId: string, result: unknown) => void;
}

/**
 * Conversation context - callers manage history externally
 */
export interface ConversationContext {
  /** Unique conversation ID for tracking/logging */
  conversationId: string;
  /** Conversation history (managed by caller) */
  history: Message[];
}

export class VoicePipeline {
  private stt: STTPipeline | null;
  private llm: LLMPipeline;
  private tts: TTSPipeline | null;
  private systemPrompt: string;
  private textNormalizer = new TextNormalizer();
  private tools: Map<string, Tool> = new Map();
  private toolDefinitions: ToolDefinition[] = [];
  private toolFillerPhrases: string[];
  private fillerPhraseIndex = 0;

  constructor(config: VoicePipelineConfig) {
    this.stt = config.stt ?? null;
    this.llm = config.llm;
    this.tts = config.tts ?? null;
    this.systemPrompt = config.systemPrompt;
    this.toolFillerPhrases = config.toolFillerPhrases ?? DEFAULT_TOOL_FILLER_PHRASES;

    // Register tools
    if (config.tools) {
      for (const tool of config.tools) {
        this.registerTool(tool);
      }
    }
  }

  /**
   * Register a tool for function calling
   */
  registerTool(tool: Tool): void {
    this.tools.set(tool.name, tool);
    this.toolDefinitions.push({
      name: tool.name,
      description: tool.description,
      parameters: tool.parameters,
    });
  }

  /**
   * Unregister a tool
   */
  unregisterTool(name: string): void {
    this.tools.delete(name);
    this.toolDefinitions = this.toolDefinitions.filter(t => t.name !== name);
  }

  /**
   * Get registered tools
   */
  getTools(): ToolDefinition[] {
    return [...this.toolDefinitions];
  }

  /**
   * Get the system prompt (for initializing conversation history)
   */
  getSystemPrompt(): string {
    return this.systemPrompt;
  }

  /**
   * Create initial history with system prompt
   */
  createInitialHistory(): Message[] {
    return [{ role: 'system', content: this.systemPrompt }];
  }

  async initialize(onProgress?: ProgressCallback): Promise<void> {
    const promises: Promise<void>[] = [this.llm.initialize(onProgress)];

    if (this.stt) {
      promises.push(this.stt.initialize(onProgress));
    }
    if (this.tts) {
      promises.push(this.tts.initialize(onProgress));
    }

    await Promise.all(promises);
  }

  isReady(): boolean {
    const sttReady = this.stt ? this.stt.isReady() : true;
    const ttsReady = this.tts ? this.tts.isReady() : true;
    return sttReady && this.llm.isReady() && ttsReady;
  }

  /**
   * Check if pipeline has STT configured
   */
  hasSTT(): boolean {
    return this.stt !== null;
  }

  /**
   * Check if pipeline has TTS configured
   */
  hasTTS(): boolean {
    return this.tts !== null;
  }

  /**
   * Process text input through LLM (and optionally TTS)
   * Returns new messages to append to history
   */
  async processText(
    text: string,
    context: ConversationContext,
    callbacks: Omit<VoicePipelineCallbacks, 'onTranscript'>
  ): Promise<Message[]> {
    try {
      const newMessages = await this.processTranscript(text, context, {
        ...callbacks,
        onTranscript: () => {},
      });
      callbacks.onComplete();
      return newMessages;
    } catch (error) {
      callbacks.onError(error instanceof Error ? error : new Error(String(error)));
      return [];
    }
  }

  /**
   * Process audio input through STT → LLM → TTS
   * Returns new messages to append to history
   */
  async processAudio(
    audio: Float32Array,
    context: ConversationContext,
    callbacks: VoicePipelineCallbacks
  ): Promise<Message[]> {
    if (!this.stt) {
      callbacks.onError(new Error('No STT backend configured. Use processText() instead.'));
      return [];
    }

    try {
      const transcript = await this.stt.transcribe(audio);
      if (!transcript.trim()) {
        callbacks.onError(new Error('Could not transcribe audio'));
        return [];
      }
      callbacks.onTranscript(transcript);

      const newMessages = await this.processTranscript(transcript, context, callbacks);
      callbacks.onComplete();
      return newMessages;
    } catch (error) {
      callbacks.onError(error instanceof Error ? error : new Error(String(error)));
      return [];
    }
  }

  /**
   * Internal: Process transcript through LLM
   */
  private async processTranscript(
    transcript: string,
    context: ConversationContext,
    callbacks: VoicePipelineCallbacks
  ): Promise<Message[]> {
    const newMessages: Message[] = [];

    // Add user message to context history
    const userMessage: Message = { role: 'user', content: transcript };
    context.history.push(userMessage);
    newMessages.push(userMessage);

    // Generate response with context
    const responseMessages = await this.generateResponse(context, callbacks);
    newMessages.push(...responseMessages);

    return newMessages;
  }

  /**
   * Internal: Generate LLM response (with tool loop)
   */
  private async generateResponse(
    context: ConversationContext,
    callbacks: VoicePipelineCallbacks
  ): Promise<Message[]> {
    const newMessages: Message[] = [];
    const useNativeTools = (this.llm.supportsTools?.() ?? false) && this.toolDefinitions.length > 0;
    const hasTools = this.toolDefinitions.length > 0;

    // Tool execution loop
    for (let iteration = 0; iteration < MAX_TOOL_ITERATIONS; iteration++) {
      const isToolCheckTurn = hasTools && iteration === 0;
      // Native tools now support streaming (via <text_response> format)
      // So we can stream whenever it's appropriate
      const shouldStream = !isToolCheckTurn || useNativeTools;

      const result = await this.generateLLMResponse(
        context,
        callbacks,
        useNativeTools,
        shouldStream  // Enable streaming for native tools too
      );

      const toolCalls = useNativeTools
        ? result.toolCalls
        : this.parsePromptBasedToolCalls(result.content);

      if (!toolCalls || toolCalls.length === 0) {
        // Only call streamResponse if we didn't stream during generation
        // Native tools stream during generation, so skip here
        if (!shouldStream) {
          await this.streamResponse(result.content, callbacks);
        }

        const assistantMsg: Message = { role: 'assistant', content: result.content };
        context.history.push(assistantMsg);
        newMessages.push(assistantMsg);
        return newMessages;
      }

      // Tool call - say filler phrase
      if (this.toolFillerPhrases.length > 0) {
        const fillerPhrase = this.toolFillerPhrases[this.fillerPhraseIndex % this.toolFillerPhrases.length];
        this.fillerPhraseIndex++;
        await this.streamResponse(fillerPhrase + ' ', callbacks);
      }

      const assistantContent = useNativeTools ? result.content : '';
      const assistantMsg: AssistantMessage = {
        role: 'assistant',
        content: assistantContent,
        toolCalls,
      };
      context.history.push(assistantMsg);
      newMessages.push(assistantMsg);

      // Execute tools
      for (const toolCall of toolCalls) {
        callbacks.onToolCall?.(toolCall);

        const tool = this.tools.get(toolCall.name);
        if (!tool) {
          const errorMsg: ToolMessage = {
            role: 'tool',
            toolCallId: toolCall.id,
            content: JSON.stringify({ error: `Unknown tool: ${toolCall.name}` }),
          };
          context.history.push(errorMsg);
          newMessages.push(errorMsg);
          callbacks.onToolResult?.(toolCall.id, { error: `Unknown tool: ${toolCall.name}` });
          continue;
        }

        try {
          const toolResult = await tool.execute(toolCall.arguments);
          const resultMsg: ToolMessage = {
            role: 'tool',
            toolCallId: toolCall.id,
            content: JSON.stringify(toolResult),
          };
          context.history.push(resultMsg);
          newMessages.push(resultMsg);
          callbacks.onToolResult?.(toolCall.id, toolResult);
        } catch (error) {
          const errorResult = { error: error instanceof Error ? error.message : String(error) };
          const errorMsg: ToolMessage = {
            role: 'tool',
            toolCallId: toolCall.id,
            content: JSON.stringify(errorResult),
          };
          context.history.push(errorMsg);
          newMessages.push(errorMsg);
          callbacks.onToolResult?.(toolCall.id, errorResult);
        }
      }
    }

    console.warn('VoicePipeline: Max tool iterations reached');
    return newMessages;
  }

  /**
   * Stream a response to the client with optional TTS
   */
  private async streamResponse(content: string, callbacks: VoicePipelineCallbacks): Promise<void> {
    if (!content) return;

    // Stream text chunks
    callbacks.onResponseChunk(content);

    // TTS if available
    if (this.tts) {
      const normalizedText = this.textNormalizer.normalize(content);
      if (normalizedText) {
        const playable = await this.tts.synthesize(normalizedText);
        callbacks.onAudio(playable);
      }
    }
  }

  /**
   * Generate LLM response
   */
  private async generateLLMResponse(
    context: ConversationContext,
    callbacks: VoicePipelineCallbacks,
    useNativeTools: boolean,
    shouldStream: boolean
  ): Promise<{ content: string; toolCalls?: ToolCall[] }> {
    const tools = this.toolDefinitions.length > 0 ? this.toolDefinitions : undefined;

    if (shouldStream && this.tts) {
      return this.generateWithStreamingTTS(context, callbacks, useNativeTools);
    }

    if (shouldStream) {
      const result = await this.llm.generate(context.history, {
        tools,
        conversationId: context.conversationId,
        onToken: (token) => callbacks.onResponseChunk(token),
      });
      return { content: result.content, toolCalls: result.toolCalls };
    }

    const result = await this.llm.generate(context.history, {
      tools,
      conversationId: context.conversationId,
    });
    return { content: result.content, toolCalls: result.toolCalls };
  }

  /**
   * Generate with streaming TTS (sentence by sentence)
   */
  private async generateWithStreamingTTS(
    context: ConversationContext,
    callbacks: VoicePipelineCallbacks,
    _useNativeTools: boolean
  ): Promise<{ content: string; toolCalls?: ToolCall[] }> {
    const tools = this.toolDefinitions.length > 0 ? this.toolDefinitions : undefined;
    let sentenceBuffer = '';
    const sentenceEnders = /[.!?]/;
    const playableQueue = new Map<number, AudioPlayable>();
    let nextSentenceIndex = 0;
    let nextToSend = 0;
    const ttsPromises: Promise<void>[] = [];

    const flushPlayableQueue = () => {
      while (playableQueue.has(nextToSend)) {
        const playable = playableQueue.get(nextToSend)!;
        callbacks.onAudio(playable);
        playableQueue.delete(nextToSend);
        nextToSend++;
      }
    };

    const queueTTS = (sentence: string, index: number) => {
      const normalizedText = this.textNormalizer.normalize(sentence);
      const promise = this.tts!
        .synthesize(normalizedText)
        .then((playable) => {
          playableQueue.set(index, playable);
          flushPlayableQueue();
        })
        .catch(() => {
          nextToSend = Math.max(nextToSend, index + 1);
          flushPlayableQueue();
        });
      ttsPromises.push(promise);
    };

    const result = await this.llm.generate(context.history, {
      tools,
      conversationId: context.conversationId,
      onToken: (token) => {
        callbacks.onResponseChunk(token);
        sentenceBuffer += token;

        const match = sentenceBuffer.match(sentenceEnders);
        if (match && match.index !== undefined) {
          const sentence = sentenceBuffer.slice(0, match.index + 1).trim();
          sentenceBuffer = sentenceBuffer.slice(match.index + 1);
          if (sentence) {
            queueTTS(sentence, nextSentenceIndex++);
          }
        }
      },
    });

    if (sentenceBuffer.trim()) {
      queueTTS(sentenceBuffer.trim(), nextSentenceIndex++);
    }

    if (ttsPromises.length > 0) {
      await Promise.all(ttsPromises);
    }

    return { content: result.content, toolCalls: result.toolCalls };
  }

  /**
   * Parse tool calls from LLM output (for non-native tool backends)
   */
  private parsePromptBasedToolCalls(content: string): ToolCall[] | undefined {
    if (this.toolDefinitions.length === 0) {
      return undefined;
    }

    // Check if the content looks like a tool call (starts with { and contains "tool_call")
    const trimmed = content.trim();
    if (!trimmed.startsWith('{') || !trimmed.includes('"tool_call"')) {
      return undefined;
    }

    // Try to parse the entire content as a tool call JSON
    try {
      const parsed = JSON.parse(trimmed);
      if (parsed.tool_call?.name) {
        return [{
          id: `prompt-${Date.now()}-${Math.random().toString(36).slice(2, 9)}`,
          name: parsed.tool_call.name,
          arguments: parsed.tool_call.arguments || {},
        }];
      }
    } catch {
      // Not valid JSON, try to extract tool call with regex
    }

    // Fallback: try to extract JSON object from content
    // This handles cases where there's extra text around the JSON
    const jsonMatch = content.match(/\{[\s\S]*"tool_call"[\s\S]*\}/);
    if (jsonMatch) {
      try {
        const parsed = JSON.parse(jsonMatch[0]);
        if (parsed.tool_call?.name) {
          return [{
            id: `prompt-${Date.now()}-${Math.random().toString(36).slice(2, 9)}`,
            name: parsed.tool_call.name,
            arguments: parsed.tool_call.arguments || {},
          }];
        }
      } catch {
        // Skip malformed JSON
      }
    }

    return undefined;
  }
}
